{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mexican federal budget pre-processing pipeline\n",
    "\n",
    "## Instructions\n",
    "\n",
    "To you run the notebook:\n",
    "\n",
    "1. choose a unique `ITERATION_LABEL` for each pipeline run\n",
    "2. specify and describe your input files (`INPUT_FILES`)\n",
    "3. make sure your column mapping (`COLUMN_ALIASES`) is correct\n",
    "3. run the whole notebook by clicking on __Kernel > Restart & Run All__\n",
    "\n",
    "## Settings\n",
    "\n",
    "Choose a unique iteration label for each pipeline run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ITERATION_LABEL = 'iteration-6-development'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Put your input files inside the `pipeline.in` folder and describe them here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "INPUT_FILES = {\n",
    "    2010: {'name': 'Cuenta_Publica_2010.csv', 'encoding': 'windows-1252'},\n",
    "#     2011: {'name': 'Cuenta_Publica_2011.csv', 'encoding': 'windows-1252'},\n",
    "#     2012: {'name': 'Cuenta_Publica_2012.csv', 'encoding': 'windows-1252'},\n",
    "#     2013: {'name': 'Cuenta_Publica_2013.csv', 'encoding': 'windows-1252'},\n",
    "#     2014: {'name': 'Cuenta_Publica_2014.csv', 'encoding': 'windows-1252'},\n",
    "#     2015: {'name': 'Cuenta_Publica_2015.csv', 'encoding': 'windows-1252'},\n",
    "#     2016: {'name': 'PEF2016_AC01.csv', 'encoding': 'cp850'}\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your input files don't all have the same column names, define your mapping here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "COLUMN_ALIASES = {\n",
    "    'Actividad Institucional': ['AI'],\n",
    "    'Adefas': ['ADEFAS'],\n",
    "    'Aprobado': [\n",
    "        'PEF_2016',\n",
    "        'Importe Presupuesto de Egresos de la Federación',\n",
    "        'Importe Presupuesto de Egresos de la Federación (PEF)'\n",
    "    ],\n",
    "    'Ciclo': None,\n",
    "    'Clave de cartera': ['CLAVE_CARTERA'],\n",
    "    'Descripción de Fuente de Financiamiento': ['FUENTE_FINAN_DESCRIPCION'],\n",
    "    'Descripción de Función': ['FUNCIONL_DESCRIPCION'],\n",
    "    'Descripción de Grupo Funcional': [\n",
    "        'Descripción de Finalidad',\n",
    "        'GRUPO_FUN_DESCRIPCION',\n",
    "        'Descripción de Grupo Funcional'\n",
    "    ],\n",
    "    'Descripción de Objeto del Gasto': ['CONCEPTO_DESCRIPCION'],\n",
    "    'Descripción de Programa Presupuestario': ['PROGR_PRES_DESCRIPCION'],\n",
    "    'Descripción de Ramo': ['RAMO_DESCRIPCION'],\n",
    "    'Descripción de Reasignacion': ['REASIGNACION_DESCRIPCION'],\n",
    "    'Descripción de Subfunción': ['SUBFUNCIONL_DESCRIPCION'],\n",
    "    'Descripción de Tipo de Gasto': ['TIPO_GASTO_DESCRIPCION'],\n",
    "    'Descripción de Unidad Responsable': ['UNIDAD_DESCRIPCION'],\n",
    "    'Descripción de la Actividad Institucional': [\n",
    "        'ACTIVIDAD_INST_DESCRIPCION',\n",
    "        'Descripción de Actividad Institucional'\n",
    "    ],\n",
    "    'Descripción de la entidad federativa': ['ENTIDAD_FED_DESCRIPCION'],\n",
    "    'Descripción de la modalidad del programa presupuestario': [\n",
    "        'MODALIDAD_DESCRIPCION',\n",
    "        'Descripción del Identificador del Programa Presupuestario',\n",
    "        'Descripción del Identificador de Programa Presupuestario'\n",
    "    ],\n",
    "    'Devengado': None,\n",
    "    'Ejercicio': None,\n",
    "    'Ejercido': None,\n",
    "    'Entidad Federativa': ['EF'],\n",
    "    'Fuente de Financiamiento': ['FF'],\n",
    "    'Función': ['FN'],\n",
    "    'Grupo Funcional': [\n",
    "        'Finalidad', 'GF', 'Grupo Funcional'\n",
    "    ],\n",
    "    'Modalidad del Programa presupuestario': [\n",
    "        'MOD',\n",
    "        'Identificador de Programa Presupuestario',\n",
    "        'Identificador del Programa Presupuestario'\n",
    "    ],\n",
    "    'Modificado': None,\n",
    "    'Objeto del Gasto': ['CONCEPTO'],\n",
    "    'Pagado': None,\n",
    "    'Programa Presupuestario': ['PP'],\n",
    "    'Ramo': None,\n",
    "    'Reasignacion': ['RA'],\n",
    "    'Subfunción': ['SF'],\n",
    "    'Tipo de Gasto': ['TG'],\n",
    "    'Unidad Responsable': ['UNIDAD']\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following hierarchical categories will have IDs prefixed with the parent categories:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "HIERARCHIES = {\n",
    "    'functional': [\n",
    "        'Grupo Funcional', \n",
    "        'Función', \n",
    "        'Subfunción', \n",
    "        'Actividad Institucional'\n",
    "    ],\n",
    "#     'administrative': [\n",
    "#         'Ramo', \n",
    "#         'Unidad Responsable'\n",
    "#     ],\n",
    "#     'activities': [\n",
    "#         'Modalidad del Programa presupuestario', \n",
    "#         'Programa Presupuestario'\n",
    "#     ],\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's it. Now just run the notebook from beginning to end."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sys import stdout\n",
    "from pandas import read_csv, concat, DataFrame, ExcelWriter, ExcelFile, Series\n",
    "from numpy import nan\n",
    "from os.path import join, isdir\n",
    "from os import mkdir\n",
    "from json import dumps\n",
    "from pprint import pprint\n",
    "from slugify import slugify"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "BASENAME = 'mexican_federal_budget'\n",
    "INPUT_FOLDER = 'pipeline.in'\n",
    "OUTPUT_FOLDER = 'pipeline.out'\n",
    "ITERATION_FOLDER = join(OUTPUT_FOLDER, ITERATION_LABEL)\n",
    "MERGED_FILE = join(ITERATION_FOLDER, BASENAME + '.merged.csv')\n",
    "CATALOGS_FOLDER = 'objeto_del_gasto.catalog'\n",
    "CATALOGS_FILE = 'objeto_del_gasto.catalog.xlsx'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if isdir(ITERATION_FOLDER):\n",
    "    raise ValueError('Please enter a unique iteration label')\n",
    "    \n",
    "mkdir(ITERATION_FOLDER)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding inspection\n",
    "\n",
    "Detect the file encodings of the input files using the `cChardet` utility library. __Warning:__ it's not always accurate. This is meant only as an indication only. In the end, encodings will be taken from `INPUT_FILES`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def detect_encodings():\n",
    "    \"\"\"Detect CSV file encoding with the cChardet library\"\"\"\n",
    "\n",
    "    try:\n",
    "        import cchardet as chardet\n",
    "    except ImportError:\n",
    "        cChardet = 'https://github.com/PyYoshi/cChardet'\n",
    "        print('Encoding inspection skipped: install %s', cChardet)\n",
    "        return\n",
    "\n",
    "    results = {}\n",
    "    results_file = join(OUTPUT_FOLDER, ITERATION_LABEL, 'encodings.detected.json')\n",
    "    \n",
    "    for year, file in sorted(INPUT_FILES.items()):\n",
    "        datafile = join(INPUT_FOLDER, file['name'])\n",
    "        \n",
    "        with open(datafile, 'rb') as f:\n",
    "            text = f.read()\n",
    "            \n",
    "        result = chardet.detect(text)\n",
    "        results.update({year: result})\n",
    "        print(year, 'Inspected', file['name'], result)\n",
    "    \n",
    "    with open(results_file, 'w+') as json:\n",
    "        json.write(dumps(results, indent=4))\n",
    "        print('\\nSaved encoding detection report to', results_file)\n",
    "        \n",
    "# detect_encodings()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def read_columns(file, encoding):\n",
    "    \"\"\"Return clean CSV file headers\"\"\"\n",
    "    \n",
    "    with open(file, encoding=encoding) as csv:\n",
    "        header = csv.readline()\n",
    "        return header.replace('\\n', '').split(',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def force_strings(columns):\n",
    "    \"\"\"Return string enforcement for each column of a CSV file\"\"\"\n",
    "    \n",
    "    for column in columns:\n",
    "        yield column, str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_csv_files():\n",
    "    \"\"\"Load raw data (CSV) files\"\"\"\n",
    "    \n",
    "    batch = {}\n",
    "    \n",
    "    for year, file in sorted(INPUT_FILES.items()):\n",
    "        filepath = join(INPUT_FOLDER, file['name'])\n",
    "        column_names = read_columns(filepath, file['encoding'])\n",
    "        column_types = dict(force_strings(column_names))\n",
    "        \n",
    "        batch[year] = read_csv(filepath, encoding=file['encoding'], dtype=column_types)\n",
    "        print('Loaded', file['name'], 'with encoding', file['encoding'])\n",
    "        stdout.flush()\n",
    "            \n",
    "    return batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def strip_cell_padding(batch):\n",
    "    for year in sorted(batch.keys()):\n",
    "        for column in batch[year].columns:\n",
    "            batch[year].rename(columns={column: column.strip()}, inplace=True)\n",
    "            batch[year][column] = batch[year][column].apply(lambda x: x.strip() if x is not nan else x)\n",
    "        print(year, 'stripped cell paddings')\n",
    "        stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def delete_empty_columns(batch):\n",
    "    for year in batch.keys():\n",
    "        for column in batch[year].columns:\n",
    "            if 'Unnamed:' in column:\n",
    "                try:\n",
    "                    del batch[year][column]\n",
    "                    print(year, column, 'deleted')\n",
    "                    stdout.flush()\n",
    "                except KeyError:\n",
    "                    pass  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def count_missing_values(batch):\n",
    "    collector = {}\n",
    "    table = []\n",
    "\n",
    "    for column in get_union_of_columns(batch):\n",
    "        row = {'Column': column}\n",
    "        collector.update({column: []})\n",
    "        \n",
    "        for year in batch.keys():\n",
    "            if column in batch[year].columns:\n",
    "                is_empty = batch[year][column].isnull()\n",
    "                empty_lines = batch[year].where(is_empty).dropna(how='all')\n",
    "                collector[column].extend(empty_lines.to_dict(orient='records'))\n",
    "                nb_empty_cells = len(empty_lines)\n",
    "            else:\n",
    "                nb_empty_cells = nan\n",
    "                \n",
    "            row.update({year: nb_empty_cells})\n",
    "            if nb_empty_cells not in (nan, 0):\n",
    "                print(year, 'found', nb_empty_cells, 'missing values in', column)\n",
    "\n",
    "        table.append(row)\n",
    "        \n",
    "    ordered_columns = ['Column']\n",
    "    ordered_columns.extend(sorted(batch.keys()))\n",
    "    empty_values_overview_table = DataFrame(table).reindex_axis(ordered_columns, axis=1)\n",
    "    \n",
    "    return empty_values_overview_table, collector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def count_duplicates(batch):\n",
    "    for year, df in sorted(batch.items()):\n",
    "        nb_duplicate_lines = df.duplicated().apply(lambda x: 1 if x is True else 0).sum()\n",
    "        print(year, 'found', nb_duplicate_lines, 'duplicate lines')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alias column names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_union_of_columns(batch):\n",
    "    union = set()\n",
    "    for year in batch.keys():\n",
    "        union = union | set(batch[year].columns)\n",
    "    return union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from yaml import load\n",
    "\n",
    "def load_aliases(file):\n",
    "    with open(file) as yaml:\n",
    "        aliases = load(yaml.read())\n",
    "        return aliases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def map_columns_to_aliases(batch, list_of_aliases):\n",
    "    for year in sorted(batch.keys()):\n",
    "        for column in sorted(batch[year].columns):\n",
    "            if not column in list_of_aliases:\n",
    "                for reference, aliases in list_of_aliases.items():\n",
    "                    if aliases:\n",
    "                        if column in aliases:\n",
    "                            batch[year].rename(columns={column: reference}, inplace=True)\n",
    "                            print(year, column, 'replaced with', reference)\n",
    "                            stdout.flush()\n",
    "                            break  \n",
    "                else:\n",
    "                    print(year, 'NO ALIAS: ', column)\n",
    "                    stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_overview(batch):\n",
    "    table = []\n",
    "    \n",
    "    for column in get_union_of_columns(batch):\n",
    "        row = {'Column': column}\n",
    "        for year in batch.keys():\n",
    "            row.update({year: column in batch[year].columns})\n",
    "        table.append(row)\n",
    "        \n",
    "    ordered_columns = ['Column']\n",
    "    ordered_columns.extend(sorted(batch.keys()))\n",
    "    \n",
    "    overview = DataFrame(table).reindex_axis(ordered_columns, axis=1)\n",
    "    return overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check expenditure sums\n",
    "\n",
    "There's a little cleaning to do on the amount columns (zeros represented by a dash). Assume thousands are seperated by a comma."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "EXPENDITURE_COLUMNS = [\n",
    "    'Ejercido', \n",
    "    'Devengado', \n",
    "    'Aprobado', \n",
    "    'Pagado', \n",
    "    'Modificado', \n",
    "    'Adefas', \n",
    "    'Ejercicio'\n",
    "]\n",
    "\n",
    "def clean_expenditure_columns(batch):\n",
    "    check_sums = []\n",
    "\n",
    "    for column in EXPENDITURE_COLUMNS:\n",
    "        row = {'Column': column}\n",
    "        \n",
    "        for year in sorted(batch.keys()):\n",
    "            try:\n",
    "                series = batch[year][column]\n",
    "                \n",
    "                # I'm assuming -' represents zero\n",
    "                series = series.apply(lambda x: '0' if x == '-' else x)\n",
    "                series = series.apply(lambda x: x.replace(',', '') if x is not nan else x)                \n",
    "                batch[year][column] = series.astype(float)\n",
    "                check_sum = batch[year][column].sum()\n",
    "                \n",
    "                print(year, 'cleaned and summed', column, '=', check_sum, 'pesos')\n",
    "                \n",
    "            except KeyError:\n",
    "                check_sum = nan\n",
    "                \n",
    "            row.update({year: check_sum})\n",
    "        \n",
    "        check_sums.append(row)\n",
    "\n",
    "    ordered_columns = ['Column']\n",
    "    ordered_columns.extend(sorted(batch.keys()))\n",
    "    return DataFrame(check_sums).reindex_axis(ordered_columns, axis=1)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objeto del Gasto Column split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from os.path import join\n",
    "\n",
    "def generate_catalog(file):\n",
    "    \n",
    "    catalog_ = {}\n",
    "    catalog_file = ExcelFile(file)\n",
    "    INDEX_COLUMN = 0\n",
    "    \n",
    "    for sheet in catalog_file.sheet_names:\n",
    "        if sheet != 'Concatenated':\n",
    "            name = slugify(sheet, separator='_')\n",
    "            output = join('objeto_del_gasto.catalog', name + '.csv')\n",
    "\n",
    "            df = catalog_file.parse(sheet).dropna()\n",
    "            index = df.columns[INDEX_COLUMN]\n",
    "\n",
    "            df[index] =  df[index].astype(str)\n",
    "            df.set_index(index, inplace=True)\n",
    "            df = df.groupby(df.index).first()\n",
    "            df.sort_index(inplace=True)\n",
    "            \n",
    "            message = 'Loaded catalog {sheet} into \"{name}\" ({nb} lines)'\n",
    "            parameters = dict(sheet=sheet, name=name, nb=len(df))\n",
    "\n",
    "            print(message.format(**parameters))\n",
    "            catalog_[name] = df['DESCRIPCION']\n",
    "    \n",
    "    print()\n",
    "    return catalog_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded catalog CAPITULO into \"capitulo\" (9 lines)\n",
      "Loaded catalog CONCEPTO into \"concepto\" (63 lines)\n",
      "Loaded catalog PARTIDA GENERICA into \"partida_generica\" (354 lines)\n",
      "Loaded catalog PARTIDA ESPECÍFICA into \"partida_especifica\" (668 lines)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CONCEPTO\n",
       "1100    Remuneraciones al personal de carácter permanente\n",
       "1200    Remuneraciones al personal de carácter transit...\n",
       "1300              Remuneraciones adicionales y especiales\n",
       "1400                                     Seguridad social\n",
       "1500             Otras prestaciones sociales y económicas\n",
       "1600                                          Previsiones\n",
       "1700              Pago de estímulos a servidores públicos\n",
       "2100    Materiales de administracion, emision de docum...\n",
       "2200                               Alimentos y utensilios\n",
       "2300    Materias primas y materiales de produccion y c...\n",
       "2400    Materiales y articulos de construccion y de re...\n",
       "2500    Productos químicos, farmacéuticos y de laborat...\n",
       "2600                 Combustibles, lubricantes y aditivos\n",
       "2700    Vestuario, blancos, prendas de protección y ar...\n",
       "2800              Materiales y suministros para seguridad\n",
       "2900       Herramientas, refacciones y accesorios menores\n",
       "3100                                    Servicios basicos\n",
       "3200                           Servicios de arrendamiento\n",
       "3300    Servicios profesionales, cientificos, tecnicos...\n",
       "3400       Servicios financieros, bancarios y comerciales\n",
       "3500    Servicios de instalacion, reparacion, mantenim...\n",
       "3600        Servicios de comunicación social y publicidad\n",
       "3700                     Servicios de traslado y viáticos\n",
       "3800                                  Servicios oficiales\n",
       "3900                            Otros servicios generales\n",
       "4100    Transferencias internas y asignaciones al sect...\n",
       "4200           Transferencias al resto del sector publico\n",
       "4300                             Subsidios y subvenciones\n",
       "4400                                      Ayudas sociales\n",
       "4500                             Pensiones y jubilaciones\n",
       "                              ...                        \n",
       "4900                           Transferencias al exterior\n",
       "5100                Mobiliario y equipo de administración\n",
       "5200         Mobiliario y equipo educacional y recreativo\n",
       "5300        Equipo e instrumental medico y de laboratorio\n",
       "5400                     Vehículos y equipo de transporte\n",
       "5500                        Equipo de defensa y seguridad\n",
       "5600             Maquinaria, otros equipos y herramientas\n",
       "5700                                   Activos biologicos\n",
       "5800                                     Bienes inmuebles\n",
       "5900                                  Activos intangibles\n",
       "6100            Obra publica en bienes de dominio publico\n",
       "6200                       Obra publica en bienes propios\n",
       "6300          Proyectos productivos y acciones de fomento\n",
       "7100    Inversiones para el fomento de actividades pro...\n",
       "7200                Acciones y participaciones de capital\n",
       "7300                          Compra de titulos y valores\n",
       "7400                               Concesión de prestamos\n",
       "7500    Inversiones en fideicomisos, mandatos y otros ...\n",
       "7600                        Otras inversiones financieras\n",
       "7900    Provisiones para contingencias y otras erogaci...\n",
       "8100                                      Participaciones\n",
       "8300                                         Aportaciones\n",
       "8500                                            Convenios\n",
       "9100                     Amortización de la deuda pública\n",
       "9200                        Intereses de la deuda publica\n",
       "9300                       Comisiones de la deuda publica\n",
       "9400                           Gastos de la deuda publica\n",
       "9500                                 Costo por coberturas\n",
       "9600                                   Apoyos financieros\n",
       "9900    Adeudos de ejercicios fiscales anteriores (ADE...\n",
       "Name: DESCRIPCION, dtype: object"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = generate_catalog(CATALOGS_FILE)\n",
    "c['concepto']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def split_objeto_del_gasto(batch):\n",
    "    catalog = generate_catalog(CATALOGS_FILE)\n",
    "    missing_in_catalog = []\n",
    "\n",
    "    def has_digits(n, N):\n",
    "        return n is not nan and len(n) >= N \n",
    "\n",
    "    def lookup(n, table, year):\n",
    "        try:\n",
    "            return catalog[table].loc[n]\n",
    "        except KeyError:\n",
    "            missing_in_catalog.append({'year': year, 'table': table, 'ID': n})\n",
    "            return nan\n",
    "        except TypeError:\n",
    "            # n is nan\n",
    "            return nan\n",
    "\n",
    "\n",
    "    for year in sorted(batch.keys()):\n",
    "        print(year, 'splitting objeto del gasto column')\n",
    "        objeto = batch[year]['Objeto del Gasto'].astype(str)\n",
    "        \n",
    "        batch[year]['Capitulo'] = objeto.apply(lambda x: x[0] + '000' if x not in (nan, 'nan') else nan)\n",
    "        batch[year]['Concepto'] = objeto.apply(lambda x: x[:2] + '00' if x not in (nan, 'nan') else nan)\n",
    "        batch[year]['Partida Genérica'] = objeto.apply(lambda x: x[:3] if has_digits(x, 4) else nan)\n",
    "        batch[year]['Partida Específica'] = objeto.apply(lambda x: x if has_digits(x, 5) else nan)\n",
    "\n",
    "        batch[year]['Descripción de Capitulo'] = batch[year]['Capitulo'].map(lambda x: lookup(x, 'capitulo', year))  \n",
    "        batch[year]['Descripción de Concepto'] = batch[year]['Concepto'].map(lambda x: lookup(x, 'concepto', year))  \n",
    "        batch[year]['Descripción de Partida Genérica'] = batch[year]['Partida Genérica'].map(lambda x: lookup(x, 'partida_generica', year))  \n",
    "        batch[year]['Descripción de Partida Específica'] = batch[year]['Partida Específica'].map(lambda x: lookup(x, 'partida_especifica', year))  \n",
    "        \n",
    "    return DataFrame(missing_in_catalog).drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prefix IDs \n",
    "Disambiguating sub-categories may require prefixing their IDs with their parents' IDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prefix_ids(batch):\n",
    "    for year in batch.keys():       \n",
    "        for hierarchy, levels in HIERARCHIES.items():\n",
    "            prefix = batch[year]['Ciclo'].apply(lambda x: '')\n",
    "            for n, level in enumerate(levels):\n",
    "                dash = '-' if n > 0 else ''\n",
    "                prefix = prefix + dash + batch[year][level]  \n",
    "                batch[year][level] = prefix\n",
    "                \n",
    "                print(year, 'prefixed', hierarchy, 'level', n, level)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def do_pipeline():\n",
    "\n",
    "    def echo_section(section):\n",
    "        print('\\n', section, '\\n')\n",
    "\n",
    "    echo_section('Loading files')\n",
    "    datasets = load_csv_files()\n",
    "    \n",
    "    echo_section('Delete empty columns')\n",
    "    delete_empty_columns(datasets)\n",
    "\n",
    "    echo_section('Stripping padding from cells')\n",
    "    strip_cell_padding(datasets)\n",
    "    \n",
    "    echo_section('Counting duplicate lines (NOT de-duplicating)')\n",
    "    count_duplicates(datasets)\n",
    "    \n",
    "    echo_section('Mapping column to aliases')\n",
    "    map_columns_to_aliases(datasets, COLUMN_ALIASES)\n",
    "\n",
    "    echo_section('Counting missing values')\n",
    "    missing_values_report, bad_records = count_missing_values(datasets)\n",
    "    \n",
    "    echo_section('Building column mapping overview')\n",
    "    column_mapping_report = build_overview(datasets)\n",
    "    \n",
    "    echo_section('Cleaning expenditure columns')\n",
    "    sums_report = clean_expenditure_columns(datasets)\n",
    "    \n",
    "    echo_section('Breaking down Objeto del Gasto column')\n",
    "    missing_catalog_ids = split_objeto_del_gasto(datasets)\n",
    "        \n",
    "    echo_section('Prefixing IDs of certain category hierarchies')\n",
    "    prefix_ids(datasets)\n",
    "\n",
    "    echo_section('Saving pipeline configuration')\n",
    "\n",
    "    reports_file = join(ITERATION_FOLDER, BASENAME + '.reports.xlsx')\n",
    "    writer = ExcelWriter(reports_file)    \n",
    "    missing_values_report.to_excel(writer, 'missing values', encoding='utf-8', index=False)\n",
    "    column_mapping_report.to_excel(writer, 'column mapping', encoding='utf-8', index=False)\n",
    "    sums_report.to_excel(writer, 'check sums', encoding='utf-8', index=False)\n",
    "    missing_catalog_ids.to_excel(writer, 'missing_catalog_IDs', encoding='utf-8', index=False)    \n",
    "    print('Saved 4 reports to', reports_file)    \n",
    "\n",
    "    aliases_file = join(ITERATION_FOLDER, BASENAME + '.aliases.json')\n",
    "    inputs_file = join(ITERATION_FOLDER, BASENAME + '.inputs.json')\n",
    "    levels_file = join(ITERATION_FOLDER, BASENAME + '.levels.json')\n",
    "    bad_records_file = join(ITERATION_FOLDER, BASENAME + '.missing.json')\n",
    "\n",
    "    with open(bad_records_file, 'w+') as json:\n",
    "        json.write(dumps(bad_records, indent=4))\n",
    "        \n",
    "    with open(aliases_file, 'w+') as json:\n",
    "        json.write(dumps(COLUMN_ALIASES, indent=4))\n",
    "        \n",
    "    with open(levels_file, 'w+') as json:\n",
    "        json.write(dumps(HIERARCHIES, indent=4))\n",
    "        \n",
    "    with open(inputs_file, 'w+') as json:\n",
    "        json.write(dumps(INPUT_FILES, indent=4))\n",
    "    \n",
    "    print('Saved input configuration to', inputs_file)    \n",
    "    print('Saved column aliases to', aliases_file) \n",
    "    print('Saved bad records (those with empty cells) to', bad_records_file)    \n",
    "    print('Saved hierarchy levels used for prefixing to', levels_file) \n",
    "    \n",
    "    echo_section('Pipeline run \"%s\" done' % ITERATION_LABEL)\n",
    "\n",
    "    return missing_catalog_ids, column_mapping_report, missing_values_report, sums_report, datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Loading files \n",
      "\n",
      "Loaded Cuenta_Publica_2010.csv with encoding windows-1252\n",
      "\n",
      " Delete empty columns \n",
      "\n",
      "\n",
      " Stripping padding from cells \n",
      "\n",
      "2010 stripped cell paddings\n",
      "\n",
      " Counting duplicate lines (NOT de-duplicating) \n",
      "\n",
      "2010 found 34 duplicate lines\n",
      "\n",
      " Mapping column to aliases \n",
      "\n",
      "2010 Descripción de Finalidad replaced with Descripción de Grupo Funcional\n",
      "2010 Finalidad replaced with Grupo Funcional\n",
      "\n",
      " Counting missing values \n",
      "\n",
      "\n",
      " Building column mapping overview \n",
      "\n",
      "\n",
      " Cleaning expenditure columns \n",
      "\n",
      "2010 cleaned and summed Ejercido = 2.47409995292e+12 pesos\n",
      "2010 cleaned and summed Aprobado = 2.37691503329e+12 pesos\n",
      "\n",
      " Breaking down Objeto del Gasto column \n",
      "\n",
      "Loaded catalog CAPITULO into \"capitulo\" (9 lines)\n",
      "Loaded catalog CONCEPTO into \"concepto\" (63 lines)\n",
      "Loaded catalog PARTIDA GENERICA into \"partida_generica\" (354 lines)\n",
      "Loaded catalog PARTIDA ESPECÍFICA into \"partida_especifica\" (668 lines)\n",
      "\n",
      "2010 splitting objeto del gasto column\n",
      "\n",
      " Prefixing IDs of certain category hierarchies \n",
      "\n",
      "2010 prefixed functional level 0 Grupo Funcional\n",
      "2010 prefixed functional level 1 Función\n",
      "2010 prefixed functional level 2 Subfunción\n",
      "2010 prefixed functional level 3 Actividad Institucional\n",
      "\n",
      " Saving pipeline configuration \n",
      "\n",
      "Saved 4 reports to pipeline.out/iteration-6-development/mexican_federal_budget.reports.xlsx\n",
      "Saved input configuration to pipeline.out/iteration-6-development/mexican_federal_budget.inputs.json\n",
      "Saved column aliases to pipeline.out/iteration-6-development/mexican_federal_budget.aliases.json\n",
      "Saved bad records (those with empty cells) to pipeline.out/iteration-6-development/mexican_federal_budget.missing.json\n",
      "Saved hierarchy levels used for prefixing to pipeline.out/iteration-6-development/mexican_federal_budget.levels.json\n",
      "\n",
      " Pipeline run \"iteration-6-development\" done \n",
      "\n"
     ]
    }
   ],
   "source": [
    "missing_ids, column_mapping, missing_values, sums, budgets = do_pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4522"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gc import collect\n",
    "collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved pipeline.out/iteration-6-development/mexican_federal_budget.2010.csv\n"
     ]
    }
   ],
   "source": [
    "for year, budget in budgets.items():\n",
    "    filepath = MERGED_FILE.replace('merged', str(year))\n",
    "    budget.to_csv(filepath, encoding='utf-8', index=False)\n",
    "    print('Saved', filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved merged dataset to pipeline.out/iteration-6-development/mexican_federal_budget.merged.csv\n"
     ]
    }
   ],
   "source": [
    "merged = concat(list(budgets.values()))\n",
    "merged.to_csv(MERGED_FILE, encoding='utf-8', index=False)\n",
    "print('Saved merged dataset to', MERGED_FILE)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quality control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'merged_budget' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-6372e1efe306>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmerged_budget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'merged_budget' is not defined"
     ]
    }
   ],
   "source": [
    "sorted(list(merged_budget.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "merged_budget.sample(n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "objeto_breakdown = [\n",
    "    'Ciclo', \n",
    "    'Objeto del Gasto', \n",
    "    'Capitulo', 'Concepto', \n",
    "    'Partida Específica', \n",
    "    'Partida Genérica'\n",
    "]\n",
    "merged_budget[objeto_breakdown].sample(n=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('Total: missing', len(missing_ids), 'catalog IDs to breakdown the \"Objeto del Gasto\" column')\n",
    "print('Tables:', dict(missing_ids.groupby('table').count()['ID']))\n",
    "print('Years:', dict(missing_ids.groupby('year').count()['ID']))\n",
    "missing_ids.sample(n=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "list(missing_ids['ID'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
