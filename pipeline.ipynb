{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mexican federal budget pre-processing pipeline\n",
    "\n",
    "## Instructions\n",
    "\n",
    "To you run the notebook:\n",
    "\n",
    "1. choose a unique `ITERATION_LABEL` for each pipeline run\n",
    "2. specify and describe your input files (`INPUT_FILES`)\n",
    "3. make sure your column mapping (`COLUMN_ALIASES`) is correct\n",
    "3. run the whole notebook by clicking on __Kernel > Restart & Run All__\n",
    "\n",
    "## Settings\n",
    "\n",
    "Choose a unique iteration label for each pipeline run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ITERATION_LABEL = 'iteration-8-development'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Put your input files inside the `pipeline.in` folder and describe them here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "INPUT_FILES = {\n",
    "    2010: {'name': 'Cuenta_Publica_2010.csv', 'encoding': 'windows-1252'},\n",
    "    2011: {'name': 'Cuenta_Publica_2011.csv', 'encoding': 'windows-1252'},\n",
    "    2012: {'name': 'Cuenta_Publica_2012.csv', 'encoding': 'windows-1252'},\n",
    "    2013: {'name': 'Cuenta_Publica_2013.csv', 'encoding': 'windows-1252'},\n",
    "    2014: {'name': 'Cuenta_Publica_2014.csv', 'encoding': 'windows-1252'},\n",
    "    2015: {'name': 'Cuenta_Publica_2015.csv', 'encoding': 'windows-1252'},\n",
    "    2016: {'name': '2016_2T_Gasto_OS.csv', 'encoding': 'windows-1252'} # cp850 for the original 2016 file\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your input files don't all have the same column names, define your mapping here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "COLUMN_ALIASES = {\n",
    "    'Actividad Institucional': ['AI'],\n",
    "    'Adefas': ['ADEFAS'],\n",
    "    'Aprobado': [\n",
    "        'PEF_2016',\n",
    "        'Importe Presupuesto de Egresos de la Federación',\n",
    "        'Importe Presupuesto de Egresos de la Federación (PEF)'\n",
    "    ],\n",
    "    'Ciclo': None,\n",
    "    'Clave de cartera': ['CLAVE_CARTERA'],\n",
    "    'Descripción de Fuente de Financiamiento': ['FUENTE_FINAN_DESCRIPCION'],\n",
    "    'Descripción de Función': ['FUNCIONL_DESCRIPCION'],\n",
    "    'Descripción de Grupo Funcional': [\n",
    "        'Descripción de Finalidad',\n",
    "        'GRUPO_FUN_DESCRIPCION',\n",
    "        'Descripción de Grupo Funcional'\n",
    "    ],\n",
    "    'Descripción de Objeto del Gasto': ['CONCEPTO_DESCRIPCION'],\n",
    "    'Descripción de Programa Presupuestario': ['PROGR_PRES_DESCRIPCION'],\n",
    "    'Descripción de Ramo': ['RAMO_DESCRIPCION'],\n",
    "    'Descripción de Reasignacion': ['REASIGNACION_DESCRIPCION'],\n",
    "    'Descripción de Subfunción': ['SUBFUNCIONL_DESCRIPCION', 'Descripción de subfunción'],\n",
    "    'Descripción de Tipo de Gasto': ['TIPO_GASTO_DESCRIPCION'],\n",
    "    'Descripción de Unidad Responsable': ['UNIDAD_DESCRIPCION'],\n",
    "    'Descripción de la Actividad Institucional': [\n",
    "        'ACTIVIDAD_INST_DESCRIPCION',\n",
    "        'Descripción de Actividad Institucional'\n",
    "    ],\n",
    "    'Descripción de Entidad Federativa': ['Descripción de la entidad federativa', 'ENTIDAD_FED_DESCRIPCION'],\n",
    "    'Descripción de la modalidad del programa presupuestario': [\n",
    "        'MODALIDAD_DESCRIPCION',\n",
    "        'Descripción del Identificador del Programa Presupuestario',\n",
    "        'Descripción del Identificador de Programa Presupuestario'\n",
    "    ],\n",
    "    'Devengado': None,\n",
    "    'Ejercicio': None,\n",
    "    'Ejercido': None,\n",
    "    'Entidad Federativa': ['EF'],\n",
    "    'Fuente de Financiamiento': ['FF', 'Fuente de Finaciamiento'],\n",
    "    'Función': ['FN'],\n",
    "    'Grupo Funcional': [\n",
    "        'Finalidad', 'GF', 'Grupo Funcional'\n",
    "    ],\n",
    "    'Modalidad del Programa presupuestario': [\n",
    "        'MOD',\n",
    "        'Identificador de Programa Presupuestario',\n",
    "        'Identificador del Programa Presupuestario'\n",
    "    ],\n",
    "    'Modificado': None,\n",
    "    'Objeto del Gasto': ['CONCEPTO'],\n",
    "    'Pagado': None,\n",
    "    'Programa Presupuestario': ['PP'],\n",
    "    'Ramo': ['RAMO'],\n",
    "    'Reasignacion': ['RA'],\n",
    "    'Subfunción': ['SF'],\n",
    "    'Tipo de Gasto': ['TG'],\n",
    "    'Unidad Responsable': ['UNIDAD'],\n",
    "    'Capitulo': None,\n",
    "    'Concepto': None,\n",
    "    'Partida Genérica': None,\n",
    "    'Partida Específica': None,\n",
    "    'Descripción de Capitulo': None,\n",
    "    'Descripción de Concepto': None,\n",
    "    'Descripción de Partida Genérica': None,\n",
    "    'Descripción de Partida Específica': ['Descripcion de Partida Específica'],    \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following hierarchical categories will have IDs prefixed with the parent categories:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "HIERARCHIES = {\n",
    "    'functional': [\n",
    "        'Grupo Funcional', \n",
    "        'Función', \n",
    "        'Subfunción', \n",
    "        'Actividad Institucional'\n",
    "    ],\n",
    "#     'administrative': [\n",
    "#         'Ramo', \n",
    "#         'Unidad Responsable'\n",
    "#     ],\n",
    "#     'activities': [\n",
    "#         'Modalidad del Programa presupuestario', \n",
    "#         'Programa Presupuestario'\n",
    "#     ],\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following columns are unsused and removed at the end of the pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "REMOVE_OUTPUT_COLUMNS = [\n",
    "    'Reasignacion',\n",
    "    'Objeto del Gasto',\n",
    "    'Descripción de Reasignacion',\n",
    "    'Descripción de Objeto del Gasto'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "REMOVE_INPUT_COLUMNS = {\n",
    "    2016: [\n",
    "        'Adefas',\n",
    "        'Partida Específica',\n",
    "        'Partida Genérica',\n",
    "        'Descripción de Partida Genérica',\n",
    "        'Descripcion de Partida Específica',\n",
    "        'Ejercicio',\n",
    "        'Devengado',\n",
    "        'Ejercido',\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's it. Now just run the notebook from beginning to end."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sys import stdout\n",
    "from pandas import read_csv, concat, DataFrame, ExcelWriter, ExcelFile, Series\n",
    "from numpy import nan, isnan\n",
    "from os.path import join, isdir\n",
    "from os import mkdir\n",
    "from json import dumps, loads\n",
    "from pprint import pprint\n",
    "from slugify import slugify"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "BASENAME = 'mexican_federal_budget'\n",
    "INPUT_FOLDER = 'pipeline.in'\n",
    "OUTPUT_FOLDER = 'pipeline.out'\n",
    "ITERATION_FOLDER = join(OUTPUT_FOLDER, ITERATION_LABEL)\n",
    "MERGED_FILE = join(ITERATION_FOLDER, BASENAME + '.merged.csv')\n",
    "CATALOGS_FOLDER = 'objeto_del_gasto.catalog'\n",
    "CATALOGS_FILE = 'objeto_del_gasto.catalog.xlsx'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if isdir(ITERATION_FOLDER):\n",
    "    raise ValueError('Please enter a unique iteration label')\n",
    "    \n",
    "mkdir(ITERATION_FOLDER)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding inspection\n",
    "\n",
    "Detect the file encodings of the input files using the `cChardet` utility library. __Warning:__ it's not always accurate. This is meant only as an indication only. In the end, encodings will be taken from `INPUT_FILES`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def detect_encodings():\n",
    "    \"\"\"Detect CSV file encoding with the cChardet library\"\"\"\n",
    "\n",
    "    try:\n",
    "        import cchardet as chardet\n",
    "    except ImportError:\n",
    "        cChardet = 'https://github.com/PyYoshi/cChardet'\n",
    "        print('Encoding inspection skipped: install %s', cChardet)\n",
    "        return\n",
    "\n",
    "    results = {}\n",
    "    results_file = join(OUTPUT_FOLDER, ITERATION_LABEL, 'encodings.detected.json')\n",
    "    \n",
    "    for year, file in sorted(INPUT_FILES.items()):\n",
    "        datafile = join(INPUT_FOLDER, file['name'])\n",
    "        \n",
    "        with open(datafile, 'rb') as f:\n",
    "            text = f.read()\n",
    "            \n",
    "        result = chardet.detect(text)\n",
    "        results.update({year: result})\n",
    "        print(year, 'Inspected', file['name'], result)\n",
    "    \n",
    "    with open(results_file, 'w+') as json:\n",
    "        json.write(dumps(results, indent=4))\n",
    "        print('\\nSaved encoding detection report to', results_file)\n",
    "        \n",
    "# detect_encodings()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def read_columns(file, encoding):\n",
    "    \"\"\"Return clean CSV file headers\"\"\"\n",
    "    \n",
    "    with open(file, encoding=encoding) as csv:\n",
    "        header = csv.readline()\n",
    "        return header.replace('\\n', '').split(',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def force_strings(columns):\n",
    "    \"\"\"Return string enforcement for each column of a CSV file\"\"\"\n",
    "    \n",
    "    for column in columns:\n",
    "        yield column, str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_csv_files():\n",
    "    \"\"\"Load raw data (CSV) files\"\"\"\n",
    "    \n",
    "    batch = {}\n",
    "    \n",
    "    for year, file in sorted(INPUT_FILES.items()):\n",
    "        filepath = join(INPUT_FOLDER, file['name'])\n",
    "        column_names = read_columns(filepath, file['encoding'])\n",
    "        column_types = dict(force_strings(column_names))\n",
    "        \n",
    "        batch[year] = read_csv(filepath, encoding=file['encoding'], dtype=column_types)\n",
    "        print('Loaded', file['name'], 'with encoding', file['encoding'])\n",
    "    \n",
    "    print()\n",
    "    stdout.flush()\n",
    "\n",
    "    for year in sorted(INPUT_FILES.keys()):\n",
    "        if year in REMOVE_INPUT_COLUMNS:\n",
    "            for column in REMOVE_INPUT_COLUMNS[year]:\n",
    "                try:\n",
    "                    del batch[year][column]\n",
    "                    print(year, 'deleted', column)\n",
    "                except KeyError:\n",
    "                    print(year, column, 'not found in', file['name'])\n",
    "\n",
    "        stdout.flush()\n",
    "\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def strip_cell_padding(batch):\n",
    "    for year in sorted(batch.keys()):\n",
    "        for column in batch[year].columns:\n",
    "            batch[year].rename(columns={column: column.strip()}, inplace=True)\n",
    "            batch[year][column] = batch[year][column].apply(lambda x: x.strip() if x is not nan else x)\n",
    "        print(year, 'stripped cell paddings')\n",
    "        stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def delete_empty_columns(batch):\n",
    "    for year in batch.keys():\n",
    "        for column in batch[year].columns:\n",
    "            if 'Unnamed:' in column:\n",
    "                try:\n",
    "                    del batch[year][column]\n",
    "                    print(year, column, 'deleted')\n",
    "                    stdout.flush()\n",
    "                except KeyError:\n",
    "                    pass  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def count_missing_values(batch):\n",
    "    collector = {}\n",
    "    table = []\n",
    "\n",
    "    for column in get_union_of_columns(batch):\n",
    "        row = {'Column': column}\n",
    "        collector.update({column: []})\n",
    "        \n",
    "        for year in batch.keys():\n",
    "            if column in batch[year].columns:\n",
    "                is_empty = batch[year][column].isnull()\n",
    "                empty_lines = batch[year].where(is_empty).dropna(how='all')\n",
    "                collector[column].extend(empty_lines.to_dict(orient='records'))\n",
    "                nb_empty_cells = len(empty_lines)\n",
    "            else:\n",
    "                nb_empty_cells = nan\n",
    "                \n",
    "            row.update({year: nb_empty_cells})\n",
    "            if nb_empty_cells not in (nan, 0):\n",
    "                print(year, 'found', nb_empty_cells, 'missing values in', column)\n",
    "\n",
    "        table.append(row)\n",
    "        \n",
    "    ordered_columns = ['Column']\n",
    "    ordered_columns.extend(sorted(batch.keys()))\n",
    "    empty_values_overview_table = DataFrame(table).reindex_axis(ordered_columns, axis=1)\n",
    "    \n",
    "    return empty_values_overview_table, collector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def count_duplicates(batch):\n",
    "    for year, df in sorted(batch.items()):\n",
    "        nb_duplicate_lines = df.duplicated().apply(lambda x: 1 if x is True else 0).sum()\n",
    "        print(year, 'found', nb_duplicate_lines, 'duplicate lines')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alias column names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_union_of_columns(batch):\n",
    "    union = set()\n",
    "    for year in batch.keys():\n",
    "        union = union | set(batch[year].columns)\n",
    "    return union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from yaml import load\n",
    "\n",
    "def load_aliases(file):\n",
    "    with open(file) as yaml:\n",
    "        aliases = load(yaml.read())\n",
    "        return aliases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def map_columns_to_aliases(batch, list_of_aliases):\n",
    "    for year in sorted(batch.keys()):\n",
    "        for column in sorted(batch[year].columns):\n",
    "            if not column in list_of_aliases:\n",
    "                for reference, aliases in list_of_aliases.items():\n",
    "                    if aliases:\n",
    "                        if column in aliases:\n",
    "                            batch[year].rename(columns={column: reference}, inplace=True)\n",
    "                            print(year, column, 'replaced with', reference)\n",
    "                            stdout.flush()\n",
    "                            break  \n",
    "                else:\n",
    "                    print(year, 'NO ALIAS REGISTERED FOR', column)\n",
    "                    stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_overview(batch):\n",
    "    table = []\n",
    "    \n",
    "    for column in get_union_of_columns(batch):\n",
    "        row = {'Column': column}\n",
    "        for year in batch.keys():\n",
    "            row.update({year: column in batch[year].columns})\n",
    "        table.append(row)\n",
    "        \n",
    "    ordered_columns = ['Column']\n",
    "    ordered_columns.extend(sorted(batch.keys()))\n",
    "    \n",
    "    overview = DataFrame(table).reindex_axis(ordered_columns, axis=1)\n",
    "    print('Column mapping overview: done')\n",
    "    return overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check expenditure sums\n",
    "\n",
    "There's a little cleaning to do on the amount columns (zeros represented by a dash). Assume thousands are seperated by a comma."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "EXPENDITURE_COLUMNS = [\n",
    "    'Ejercido', \n",
    "    'Devengado', \n",
    "    'Aprobado', \n",
    "    'Pagado', \n",
    "    'Modificado', \n",
    "    'Adefas', \n",
    "    'Ejercicio'\n",
    "]\n",
    "count = 0\n",
    "\n",
    "def clean_expenditure_columns(batch):\n",
    "    check_sums = []\n",
    "\n",
    "    for column in EXPENDITURE_COLUMNS:\n",
    "        row = {'Column': column}\n",
    "        \n",
    "        for year in sorted(batch.keys()):\n",
    "            try:\n",
    "                series = batch[year][column]\n",
    "                \n",
    "                # I'm assuming -' represents zero\n",
    "                series = series.apply(lambda x: '0' if x == '-' else x)\n",
    "                try:\n",
    "                    series = series.apply(lambda x: x.replace(',', '') if x is not nan else x)    \n",
    "                except AttributeError:\n",
    "                    if count < 10:\n",
    "                        print(year, column)\n",
    "                batch[year][column] = series.astype(float)\n",
    "                check_sum = batch[year][column].sum()\n",
    "                \n",
    "                print(year, 'cleaned and summed', column, '=', check_sum, 'pesos')\n",
    "                \n",
    "            except KeyError:\n",
    "                check_sum = nan\n",
    "                \n",
    "            row.update({year: check_sum})\n",
    "        \n",
    "        check_sums.append(row)\n",
    "\n",
    "    ordered_columns = ['Column']\n",
    "    ordered_columns.extend(sorted(batch.keys()))\n",
    "    return DataFrame(check_sums).reindex_axis(ordered_columns, axis=1)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objeto del Gasto Column split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from os.path import join\n",
    "\n",
    "def generate_catalog(file):\n",
    "    \n",
    "    catalog_ = {}\n",
    "    catalog_file = ExcelFile(file)\n",
    "    INDEX_COLUMN = 0\n",
    "    \n",
    "    for sheet in catalog_file.sheet_names:\n",
    "        if sheet != 'Concatenated':\n",
    "            name = slugify(sheet, separator='_')\n",
    "            output = join('objeto_del_gasto.catalog', name + '.csv')\n",
    "\n",
    "            df = catalog_file.parse(sheet).dropna()\n",
    "            index = df.columns[INDEX_COLUMN]\n",
    "\n",
    "            df[index] =  df[index].astype(str)\n",
    "            df.set_index(index, inplace=True)\n",
    "            df = df.groupby(df.index).first()\n",
    "            df.sort_index(inplace=True)\n",
    "            \n",
    "            message = 'Loaded catalog {sheet} into \"{name}\" ({nb} lines)'\n",
    "            parameters = dict(sheet=sheet, name=name, nb=len(df))\n",
    "\n",
    "            print(message.format(**parameters))\n",
    "            catalog_[name] = df['DESCRIPCION']\n",
    "    \n",
    "    print()\n",
    "    return catalog_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Note!__ Years are hard coded in the script below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def split_objeto_del_gasto(batch):\n",
    "    catalog = generate_catalog(CATALOGS_FILE)\n",
    "    missing_in_catalog = []\n",
    "    \n",
    "    def has_digits(n, N):\n",
    "        return not isinstance(n, float) and len(n) >= N \n",
    "            \n",
    "\n",
    "    def lookup(n, table, year):\n",
    "        try:\n",
    "            return catalog[table].loc[n]\n",
    "        except KeyError:\n",
    "            missing_in_catalog.append({'year': year, 'table': table, 'ID': n})\n",
    "            return nan\n",
    "        except TypeError:\n",
    "            # n is nan\n",
    "            return nan\n",
    "    \n",
    "    for year in sorted(batch.keys()):\n",
    "        if year == 2016:\n",
    "            print('Skipping', year, 'because the raw CSV already has the required columns')\n",
    "        \n",
    "        else:\n",
    "            objeto = batch[year]['Objeto del Gasto'].astype(str)\n",
    "\n",
    "            batch[year]['Capitulo'] = objeto.apply(lambda x: x[0] + '000' if x not in (nan, 'nan') else nan)\n",
    "            batch[year]['Concepto'] = objeto.apply(lambda x: x[:2] + '00' if x not in (nan, 'nan') else nan)\n",
    "            batch[year]['Descripción de Capitulo'] = batch[year]['Capitulo'].map(lambda x: lookup(x, 'capitulo', year))  \n",
    "            batch[year]['Descripción de Concepto'] = batch[year]['Concepto'].map(lambda x: lookup(x, 'concepto', year))  \n",
    "            \n",
    "            # Skip the LAST year of the dataset (currently 2016) it has split columns already\n",
    "            batch[year]['Partida Genérica'] = objeto.apply(lambda x: x[:3] if has_digits(x, 4) else nan)\n",
    "            batch[year]['Descripción de Partida Genérica'] = batch[year]['Partida Genérica'].map(lambda x: lookup(x, 'partida_generica', year))  \n",
    "            \n",
    "            if year not in (2008, 2009, 2010):\n",
    "                batch[year]['Partida Específica'] = objeto.apply(lambda x: x if has_digits(x, 5) else nan)\n",
    "                batch[year]['Descripción de Partida Específica'] = batch[year]['Partida Específica'].map(lambda x: lookup(x, 'partida_especifica', year) if has_digits(x, 5) else nan)  \n",
    "            else:\n",
    "                batch[year]['Partida Específica'] = nan\n",
    "                batch[year]['Descripción de Partida Específica'] = nan\n",
    "\n",
    "            print(year, 'broke down \"Objeto del Gasto\" column')\n",
    "        \n",
    "    return DataFrame(missing_in_catalog).drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prefix IDs \n",
    "Disambiguating sub-categories may require prefixing their IDs with their parents' IDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prefix_ids(batch):\n",
    "    for year in batch.keys():       \n",
    "        for hierarchy, levels in HIERARCHIES.items():\n",
    "            prefix = batch[year]['Ciclo'].apply(lambda x: '')\n",
    "            for n, level in enumerate(levels):\n",
    "                dash = '-' if n > 0 else ''\n",
    "                prefix = prefix + dash + batch[year][level]  \n",
    "                batch[year][level] = prefix\n",
    "                \n",
    "                print(year, 'prefixed', hierarchy, 'level', n, level)\n",
    "                stdout.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove unused columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def remove_unused_columns(batch):\n",
    "    for year, budget in batch.items():\n",
    "        for column in REMOVE_OUTPUT_COLUMNS:\n",
    "            try:\n",
    "                del budget[column]\n",
    "                print(year, 'deleted', column)\n",
    "            except KeyError:\n",
    "                pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def do_pipeline():\n",
    "\n",
    "    def echo_section(section):\n",
    "        print('\\n', section, '\\n')\n",
    "\n",
    "    echo_section('Loading files')\n",
    "    datasets = load_csv_files()\n",
    "    \n",
    "    echo_section('Delete empty columns')\n",
    "    delete_empty_columns(datasets)\n",
    "\n",
    "    echo_section('Stripping padding from cells')\n",
    "    strip_cell_padding(datasets)\n",
    "    \n",
    "    echo_section('Counting duplicate lines (NOT de-duplicating)')\n",
    "    count_duplicates(datasets)\n",
    "    \n",
    "    echo_section('Mapping column to aliases')\n",
    "    map_columns_to_aliases(datasets, COLUMN_ALIASES)\n",
    "\n",
    "    echo_section('Counting missing values')\n",
    "    missing_values_report, bad_records = count_missing_values(datasets)\n",
    "    \n",
    "    echo_section('Building column mapping overview')\n",
    "    column_mapping_report = build_overview(datasets)\n",
    "    \n",
    "    echo_section('Cleaning expenditure columns')\n",
    "    sums_report = clean_expenditure_columns(datasets)\n",
    "    \n",
    "    echo_section('Breaking down Objeto del Gasto column')\n",
    "    missing_catalog_ids = split_objeto_del_gasto(datasets)\n",
    "        \n",
    "    echo_section('Prefixing IDs of certain category hierarchies')\n",
    "    prefix_ids(datasets)\n",
    "\n",
    "    echo_section('Removing unused columns')\n",
    "    remove_unused_columns(datasets)\n",
    "\n",
    "    echo_section('Saving pipeline configuration')\n",
    "\n",
    "    reports_file = join(ITERATION_FOLDER, BASENAME + '.reports.xlsx')\n",
    "    writer = ExcelWriter(reports_file)    \n",
    "    missing_values_report.to_excel(writer, 'missing values', encoding='utf-8', index=False)\n",
    "    column_mapping_report.to_excel(writer, 'column mapping', encoding='utf-8', index=False)\n",
    "    sums_report.to_excel(writer, 'check sums', encoding='utf-8', index=False)\n",
    "    missing_catalog_ids.to_excel(writer, 'missing_catalog_IDs', encoding='utf-8', index=False)    \n",
    "    print('Saved 4 reports to', reports_file)    \n",
    "\n",
    "    aliases_file = join(ITERATION_FOLDER, BASENAME + '.aliases.json')\n",
    "    inputs_file = join(ITERATION_FOLDER, BASENAME + '.inputs.json')\n",
    "    levels_file = join(ITERATION_FOLDER, BASENAME + '.levels.json')\n",
    "    bad_records_file = join(ITERATION_FOLDER, BASENAME + '.missing.json')\n",
    "\n",
    "    with open(bad_records_file, 'w+') as json:\n",
    "        json.write(dumps(bad_records, indent=4))\n",
    "        \n",
    "    with open(aliases_file, 'w+') as json:\n",
    "        json.write(dumps(COLUMN_ALIASES, indent=4))\n",
    "        \n",
    "    with open(levels_file, 'w+') as json:\n",
    "        json.write(dumps(HIERARCHIES, indent=4))\n",
    "        \n",
    "    with open(inputs_file, 'w+') as json:\n",
    "        json.write(dumps(INPUT_FILES, indent=4))\n",
    "    \n",
    "    print('Saved input configuration to', inputs_file)    \n",
    "    print('Saved column aliases to', aliases_file) \n",
    "    print('Saved bad records (those with empty cells) to', bad_records_file)    \n",
    "    print('Saved hierarchy levels used for prefixing to', levels_file) \n",
    "    \n",
    "    echo_section('Pipeline run \"%s\" done' % ITERATION_LABEL)\n",
    "\n",
    "    return datasets, missing_catalog_ids, column_mapping_report, missing_values_report, sums_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Loading files \n",
      "\n",
      "Loaded Cuenta_Publica_2010.csv with encoding windows-1252\n",
      "Loaded Cuenta_Publica_2011.csv with encoding windows-1252\n",
      "Loaded Cuenta_Publica_2012.csv with encoding windows-1252\n",
      "Loaded Cuenta_Publica_2013.csv with encoding windows-1252\n",
      "Loaded Cuenta_Publica_2014.csv with encoding windows-1252\n",
      "Loaded Cuenta_Publica_2015.csv with encoding windows-1252\n",
      "Loaded 2016_2T_Gasto_OS.csv with encoding windows-1252\n",
      "\n",
      "2016 deleted Adefas\n",
      "2016 deleted Partida Específica\n",
      "2016 deleted Partida Genérica\n",
      "2016 deleted Descripción de Partida Genérica\n",
      "2016 deleted Descripcion de Partida Específica\n",
      "2016 deleted Ejercicio\n",
      "2016 deleted Devengado\n",
      "2016 deleted Ejercido\n",
      "\n",
      " Delete empty columns \n",
      "\n",
      "2011 Unnamed: 25 deleted\n",
      "2011 Unnamed: 26 deleted\n",
      "2011 Unnamed: 27 deleted\n",
      "2011 Unnamed: 28 deleted\n",
      "2011 Unnamed: 29 deleted\n",
      "2011 Unnamed: 30 deleted\n",
      "2011 Unnamed: 31 deleted\n",
      "2011 Unnamed: 32 deleted\n",
      "2011 Unnamed: 33 deleted\n",
      "2011 Unnamed: 34 deleted\n",
      "2011 Unnamed: 35 deleted\n",
      "2011 Unnamed: 36 deleted\n",
      "2011 Unnamed: 37 deleted\n",
      "2011 Unnamed: 38 deleted\n",
      "2011 Unnamed: 39 deleted\n",
      "2011 Unnamed: 40 deleted\n",
      "2011 Unnamed: 41 deleted\n",
      "\n",
      " Stripping padding from cells \n",
      "\n",
      "2010 stripped cell paddings\n",
      "2011 stripped cell paddings\n",
      "2012 stripped cell paddings\n",
      "2013 stripped cell paddings\n",
      "2014 stripped cell paddings\n",
      "2015 stripped cell paddings\n",
      "2016 stripped cell paddings\n",
      "\n",
      " Counting duplicate lines (NOT de-duplicating) \n",
      "\n",
      "2010 found 34 duplicate lines\n",
      "2011 found 37 duplicate lines\n",
      "2012 found 560 duplicate lines\n",
      "2013 found 0 duplicate lines\n",
      "2014 found 47 duplicate lines\n",
      "2015 found 16 duplicate lines\n",
      "2016 found 6138 duplicate lines\n",
      "\n",
      " Mapping column to aliases \n",
      "\n",
      "2010 Descripción de Finalidad replaced with Descripción de Grupo Funcional\n",
      "2010 Finalidad replaced with Grupo Funcional\n",
      "2011 Descripción de Finalidad replaced with Descripción de Grupo Funcional\n",
      "2011 Finalidad replaced with Grupo Funcional\n",
      "2012 Descripción de Finalidad replaced with Descripción de Grupo Funcional\n",
      "2012 Descripción de la entidad federativa replaced with Descripción de Entidad Federativa\n",
      "2012 Finalidad replaced with Grupo Funcional\n",
      "2013 Descripción de Finalidad replaced with Descripción de Grupo Funcional\n",
      "2013 Descripción de la entidad federativa replaced with Descripción de Entidad Federativa\n",
      "2013 Finalidad replaced with Grupo Funcional\n",
      "2014 ADEFAS replaced with Adefas\n",
      "2014 Descripción de la entidad federativa replaced with Descripción de Entidad Federativa\n",
      "2015 Descripción de la entidad federativa replaced with Descripción de Entidad Federativa\n",
      "2016 Descripción de Actividad Institucional replaced with Descripción de la Actividad Institucional\n",
      "2016 Descripción de subfunción replaced with Descripción de Subfunción\n",
      "2016 Fuente de Finaciamiento replaced with Fuente de Financiamiento\n",
      "2016 RAMO replaced with Ramo\n",
      "\n",
      " Counting missing values \n",
      "\n",
      "2011 found 1 missing values in Descripción de la modalidad del programa presupuestario\n",
      "2011 found 1 missing values in Modalidad del Programa presupuestario\n",
      "2011 found 1 missing values in Fuente de Financiamiento\n",
      "2011 found 1 missing values in Descripción de Objeto del Gasto\n",
      "2012 found 28 missing values in Descripción de Objeto del Gasto\n",
      "2011 found 1 missing values in Ejercido\n",
      "2011 found 1 missing values in Descripción de Programa Presupuestario\n",
      "2011 found 1 missing values in Tipo de Gasto\n",
      "2011 found 1 missing values in Descripción de Tipo de Gasto\n",
      "2011 found 1 missing values in Objeto del Gasto\n",
      "2012 found 172 missing values in Descripción de Entidad Federativa\n",
      "2011 found 1 missing values in Programa Presupuestario\n",
      "2011 found 1 missing values in Aprobado\n",
      "2012 found 1 missing values in Aprobado\n",
      "2011 found 1 missing values in Descripción de Fuente de Financiamiento\n",
      "2011 found 1 missing values in Descripción de la Actividad Institucional\n",
      "\n",
      " Building column mapping overview \n",
      "\n",
      "Column mapping overview: done\n",
      "\n",
      " Cleaning expenditure columns \n",
      "\n",
      "2010 cleaned and summed Ejercido = 2.47409995292e+12 pesos\n",
      "2011 cleaned and summed Ejercido = 2.69593040168e+12 pesos\n",
      "2012 cleaned and summed Ejercido = 2.89633102516e+12 pesos\n",
      "2013 cleaned and summed Ejercido = 3.13479744166e+12 pesos\n",
      "2013 cleaned and summed Devengado = 3.13501498641e+12 pesos\n",
      "2014 cleaned and summed Devengado = 3.42624242464e+12 pesos\n",
      "2015 cleaned and summed Devengado = 3.76199681531e+12 pesos\n",
      "2010 cleaned and summed Aprobado = 2.37691503329e+12 pesos\n",
      "2011 cleaned and summed Aprobado = 2.53828160725e+12 pesos\n",
      "2012 cleaned and summed Aprobado = 2.75486845421e+12 pesos\n",
      "2013 cleaned and summed Aprobado = 2.94349474075e+12 pesos\n",
      "2014 cleaned and summed Aprobado = 3.33425878523e+12 pesos\n",
      "2015 cleaned and summed Aprobado = 3.50846271051e+12 pesos\n",
      "2016 cleaned and summed Aprobado = 5.29682024151e+12 pesos\n",
      "2014 cleaned and summed Pagado = 3.38660877152e+12 pesos\n",
      "2015 cleaned and summed Pagado = 3.72805646035e+12 pesos\n",
      "2016 cleaned and summed Pagado = 2.70741750825e+12 pesos\n",
      "2014 cleaned and summed Modificado = 3.42717215959e+12 pesos\n",
      "2015 cleaned and summed Modificado = 3.76346698244e+12 pesos\n",
      "2016 cleaned and summed Modificado = 2.85045275607e+12 pesos\n",
      "2014 cleaned and summed Adefas = 36941614858.0 pesos\n",
      "2015 cleaned and summed Adefas = 31122648942.8 pesos\n",
      "2014 cleaned and summed Ejercicio = 3.4247737328e+12 pesos\n",
      "2015 cleaned and summed Ejercicio = 3.76042204595e+12 pesos\n",
      "\n",
      " Breaking down Objeto del Gasto column \n",
      "\n",
      "Loaded catalog CAPITULO into \"capitulo\" (9 lines)\n",
      "Loaded catalog CONCEPTO into \"concepto\" (63 lines)\n",
      "Loaded catalog PARTIDA GENERICA into \"partida_generica\" (354 lines)\n",
      "Loaded catalog PARTIDA ESPECÍFICA into \"partida_especifica\" (668 lines)\n",
      "\n",
      "2010 broke down \"Objeto del Gasto\" column\n",
      "2011 broke down \"Objeto del Gasto\" column\n",
      "2012 broke down \"Objeto del Gasto\" column\n",
      "2013 broke down \"Objeto del Gasto\" column\n",
      "2014 broke down \"Objeto del Gasto\" column\n",
      "2015 broke down \"Objeto del Gasto\" column\n",
      "Skipping 2016 because the raw CSV already has the required columns\n",
      "\n",
      " Prefixing IDs of certain category hierarchies \n",
      "\n",
      "2016 prefixed functional level 0 Grupo Funcional\n",
      "2016 prefixed functional level 1 Función\n",
      "2016 prefixed functional level 2 Subfunción\n",
      "2016 prefixed functional level 3 Actividad Institucional\n",
      "2010 prefixed functional level 0 Grupo Funcional\n",
      "2010 prefixed functional level 1 Función\n",
      "2010 prefixed functional level 2 Subfunción\n",
      "2010 prefixed functional level 3 Actividad Institucional\n",
      "2011 prefixed functional level 0 Grupo Funcional\n",
      "2011 prefixed functional level 1 Función\n",
      "2011 prefixed functional level 2 Subfunción\n",
      "2011 prefixed functional level 3 Actividad Institucional\n",
      "2012 prefixed functional level 0 Grupo Funcional\n",
      "2012 prefixed functional level 1 Función\n",
      "2012 prefixed functional level 2 Subfunción\n",
      "2012 prefixed functional level 3 Actividad Institucional\n",
      "2013 prefixed functional level 0 Grupo Funcional\n",
      "2013 prefixed functional level 1 Función\n",
      "2013 prefixed functional level 2 Subfunción\n",
      "2013 prefixed functional level 3 Actividad Institucional\n",
      "2014 prefixed functional level 0 Grupo Funcional\n",
      "2014 prefixed functional level 1 Función\n",
      "2014 prefixed functional level 2 Subfunción\n",
      "2014 prefixed functional level 3 Actividad Institucional\n",
      "2015 prefixed functional level 0 Grupo Funcional\n",
      "2015 prefixed functional level 1 Función\n",
      "2015 prefixed functional level 2 Subfunción\n",
      "2015 prefixed functional level 3 Actividad Institucional\n",
      "\n",
      " Removing unused columns \n",
      "\n",
      "2010 deleted Objeto del Gasto\n",
      "2010 deleted Descripción de Objeto del Gasto\n",
      "2011 deleted Objeto del Gasto\n",
      "2011 deleted Descripción de Objeto del Gasto\n",
      "2012 deleted Objeto del Gasto\n",
      "2012 deleted Descripción de Objeto del Gasto\n",
      "2013 deleted Objeto del Gasto\n",
      "2013 deleted Descripción de Objeto del Gasto\n",
      "2014 deleted Objeto del Gasto\n",
      "2014 deleted Descripción de Objeto del Gasto\n",
      "2015 deleted Objeto del Gasto\n",
      "2015 deleted Descripción de Objeto del Gasto\n",
      "\n",
      " Saving pipeline configuration \n",
      "\n",
      "Saved 4 reports to pipeline.out/iteration-8-development/mexican_federal_budget.reports.xlsx\n",
      "Saved input configuration to pipeline.out/iteration-8-development/mexican_federal_budget.inputs.json\n",
      "Saved column aliases to pipeline.out/iteration-8-development/mexican_federal_budget.aliases.json\n",
      "Saved bad records (those with empty cells) to pipeline.out/iteration-8-development/mexican_federal_budget.missing.json\n",
      "Saved hierarchy levels used for prefixing to pipeline.out/iteration-8-development/mexican_federal_budget.levels.json\n",
      "\n",
      " Pipeline run \"iteration-8-development\" done \n",
      "\n"
     ]
    }
   ],
   "source": [
    "budgets, missing_ids, column_mapping, missing_values, sums = do_pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2465"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gc import collect\n",
    "collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved pipeline.out/iteration-8-development/mexican_federal_budget.2016.csv\n",
      "Saved pipeline.out/iteration-8-development/mexican_federal_budget.2010.csv\n",
      "Saved pipeline.out/iteration-8-development/mexican_federal_budget.2011.csv\n",
      "Saved pipeline.out/iteration-8-development/mexican_federal_budget.2012.csv\n",
      "Saved pipeline.out/iteration-8-development/mexican_federal_budget.2013.csv\n",
      "Saved pipeline.out/iteration-8-development/mexican_federal_budget.2014.csv\n",
      "Saved pipeline.out/iteration-8-development/mexican_federal_budget.2015.csv\n"
     ]
    }
   ],
   "source": [
    "for year, budget in budgets.items():\n",
    "    filepath = MERGED_FILE.replace('merged', str(year))\n",
    "    budget.to_csv(filepath, encoding='utf-8', index=False)\n",
    "    print('Saved', filepath)\n",
    "    stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "merged = concat(list(budgets.values()))\n",
    "merged.to_csv(MERGED_FILE, encoding='utf-8', index=False)\n",
    "print('Saved merged dataset to', MERGED_FILE)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quality control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sorted(list(budget.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "budget.sample(n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "objeto_breakdown = [\n",
    "    'Ciclo', \n",
    "    'Capitulo', 'Concepto', \n",
    "    'Partida Específica', \n",
    "    'Partida Genérica'\n",
    "]\n",
    "budget[objeto_breakdown].sample(n=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('Total: missing', len(missing_ids), 'catalog IDs to breakdown the \"Objeto del Gasto\" column')\n",
    "print('Tables:', dict(missing_ids.groupby('table').count()['ID']))\n",
    "print('Years:', dict(missing_ids.groupby('year').count()['ID']))\n",
    "missing_ids.sample(n=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "column_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "missing_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "merged.sample(n=20) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(join(ITERATION_FOLDER, BASENAME + '.missing.json')) as file:\n",
    "    aliases = loads(file.read())\n",
    "aliases['Descripción de Fuente de Financiamiento']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "breakdown = [\n",
    "    'Ciclo', \n",
    "    'Capitulo', \n",
    "    'Concepto', \n",
    "    'Partida Genérica',        \n",
    "    'Partida Específica', \n",
    "    'Descripción de Capitulo',\n",
    "    'Descripción de Concepto', \n",
    "    'Descripción de Partida Genérica',\n",
    "    'Descripción de Partida Específica'\n",
    "]\n",
    "\n",
    "merged[breakdown].sample(n=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "merged.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
