{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mexican federal budget pre-processing pipeline\n",
    "\n",
    "## Instructions\n",
    "\n",
    "To you run the notebook:\n",
    "\n",
    "1. choose a unique `ITERATION_LABEL` for each pipeline run\n",
    "2. specify and describe your input files (`INPUT_FILES`)\n",
    "3. make sure your column mapping (`COLUMN_ALIASES`) is correct\n",
    "3. run the whole notebook by clicking on __Kernel > Restart & Run All__\n",
    "\n",
    "## Settings\n",
    "\n",
    "Choose a unique iteration label for each pipeline run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ITERATION_LABEL = 'iteration-9-development'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Put your input files inside the `pipeline.in` folder and describe them here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "INPUT_FILES = {\n",
    "    2008: {'name': 'Cuenta_Publica_2008.csv', 'encoding': 'windows-1252'},\n",
    "    2009: {'name': 'Cuenta_Publica_2009.csv', 'encoding': 'windows-1252'},\n",
    "    2010: {'name': 'Cuenta_Publica_2010.csv', 'encoding': 'windows-1252'},\n",
    "    2011: {'name': 'Cuenta_Publica_2011.csv', 'encoding': 'windows-1252'},\n",
    "    2012: {'name': 'Cuenta_Publica_2012.csv', 'encoding': 'windows-1252'},\n",
    "    2013: {'name': 'Cuenta_Publica_2013.csv', 'encoding': 'windows-1252'},\n",
    "    2014: {'name': 'Cuenta_Publica_2014.csv', 'encoding': 'windows-1252'},\n",
    "    2015: {'name': 'Cuenta_Publica_2015.csv', 'encoding': 'windows-1252'},\n",
    "    2016: {'name': 'Cuenta_Publica_2016.csv', 'encoding': 'windows-1252'} # cp850 for the original 2016 file\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your input files don't all have the same column names, define your mapping here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "COLUMN_ALIASES = {\n",
    "    'Actividad Institucional': ['AI'],\n",
    "    'Adefas': ['ADEFAS'],\n",
    "    'Aprobado': [\n",
    "        'PEF_2016',\n",
    "        'Importe Presupuesto de Egresos de la Federación',\n",
    "        'Importe Presupuesto de Egresos de la Federación (PEF)'\n",
    "    ],\n",
    "    'Ciclo': None,\n",
    "    'Clave de cartera': ['CLAVE_CARTERA'],\n",
    "    'Descripción de Fuente de Financiamiento': ['FUENTE_FINAN_DESCRIPCION'],\n",
    "    'Descripción de Función': ['FUNCIONL_DESCRIPCION'],\n",
    "    'Descripción de Grupo Funcional': [\n",
    "        'Descripción de Finalidad',\n",
    "        'GRUPO_FUN_DESCRIPCION',\n",
    "        'Descripción de Grupo Funcional'\n",
    "    ],\n",
    "    'Descripción de Objeto del Gasto': ['CONCEPTO_DESCRIPCION'],\n",
    "    'Descripción de Programa Presupuestario': ['PROGR_PRES_DESCRIPCION'],\n",
    "    'Descripción de Ramo': ['RAMO_DESCRIPCION'],\n",
    "    'Descripción de Reasignacion': ['REASIGNACION_DESCRIPCION'],\n",
    "    'Descripción de Subfunción': ['SUBFUNCIONL_DESCRIPCION', 'Descripción de subfunción'],\n",
    "    'Descripción de Tipo de Gasto': ['TIPO_GASTO_DESCRIPCION'],\n",
    "    'Descripción de Unidad Responsable': ['UNIDAD_DESCRIPCION'],\n",
    "    'Descripción de la Actividad Institucional': [\n",
    "        'ACTIVIDAD_INST_DESCRIPCION',\n",
    "        'Descripción de Actividad Institucional'\n",
    "    ],\n",
    "    'Descripción de Entidad Federativa': ['Descripción de la entidad federativa', 'ENTIDAD_FED_DESCRIPCION'],\n",
    "    'Descripción de la modalidad del programa presupuestario': [\n",
    "        'MODALIDAD_DESCRIPCION',\n",
    "        'Descripción del Identificador del Programa Presupuestario',\n",
    "        'Descripción del Identificador de Programa Presupuestario'\n",
    "    ],\n",
    "    'Devengado': None,\n",
    "    'Ejercicio': None,\n",
    "    'Ejercido': None,\n",
    "    'Entidad Federativa': ['EF'],\n",
    "    'Fuente de Financiamiento': ['FF', 'Fuente de Finaciamiento'],\n",
    "    'Función': ['FN'],\n",
    "    'Grupo Funcional': [\n",
    "        'Finalidad', 'GF', 'Grupo Funcional'\n",
    "    ],\n",
    "    'Modalidad del Programa presupuestario': [\n",
    "        'MOD',\n",
    "        'Identificador de Programa Presupuestario',\n",
    "        'Identificador del Programa Presupuestario'\n",
    "    ],\n",
    "    'Modificado': None,\n",
    "    'Objeto del Gasto': ['CONCEPTO'],\n",
    "    'Pagado': None,\n",
    "    'Programa Presupuestario': ['PP'],\n",
    "    'Ramo': ['RAMO'],\n",
    "    'Reasignacion': ['RA'],\n",
    "    'Subfunción': ['SF'],\n",
    "    'Tipo de Gasto': ['TG'],\n",
    "    'Unidad Responsable': ['UNIDAD'],\n",
    "    'Capitulo': None,\n",
    "    'Concepto': None,\n",
    "    'Partida Genérica': None,\n",
    "    'Partida Específica': None,\n",
    "    'Descripción de Capitulo': None,\n",
    "    'Descripción de Concepto': None,\n",
    "    'Descripción de Partida Genérica': None,\n",
    "    'Descripción de Partida Específica': ['Descripcion de Partida Específica'],    \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following hierarchical categories will have IDs prefixed with the parent categories:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "HIERARCHIES = {\n",
    "    'functional': [\n",
    "        'Grupo Funcional', \n",
    "        'Función', \n",
    "        'Subfunción', \n",
    "        'Actividad Institucional'\n",
    "    ],\n",
    "    'administrative': [\n",
    "        'Ramo', \n",
    "        'Unidad Responsable'\n",
    "    ],\n",
    "    'activities': [\n",
    "        'Modalidad del Programa presupuestario', \n",
    "        'Programa Presupuestario'\n",
    "    ],\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following columns are unsused and removed at the end of the pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "REMOVE_OUTPUT_COLUMNS = [\n",
    "    'Reasignacion',\n",
    "    'Objeto del Gasto',\n",
    "    'Descripción de Reasignacion',\n",
    "    'Descripción de Objeto del Gasto'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "REMOVE_INPUT_COLUMNS = {\n",
    "    2016: [\n",
    "        'Adefas',\n",
    "        'Partida Específica',\n",
    "        'Partida Genérica',\n",
    "        'Descripción de Partida Genérica',\n",
    "        'Descripcion de Partida Específica',\n",
    "        'Ejercicio',\n",
    "        'Devengado',\n",
    "        'Ejercido',\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's it. Now just run the notebook from beginning to end."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named 'slugify'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-d98687999cc4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mjson\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdumps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloads\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpprint\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpprint\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mslugify\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mslugify\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m: No module named 'slugify'"
     ]
    }
   ],
   "source": [
    "from sys import stdout\n",
    "from pandas import read_csv, concat, DataFrame, ExcelWriter, ExcelFile, Series\n",
    "from numpy import nan, isnan\n",
    "from os.path import join, isdir\n",
    "from os import mkdir\n",
    "from json import dumps, loads\n",
    "from pprint import pprint\n",
    "from slugify import slugify"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "BASENAME = 'mexican_federal_budget'\n",
    "INPUT_FOLDER = 'pipeline.in'\n",
    "OUTPUT_FOLDER = 'pipeline.out'\n",
    "ITERATION_FOLDER = join(OUTPUT_FOLDER, ITERATION_LABEL)\n",
    "MERGED_FILE = join(ITERATION_FOLDER, BASENAME + '.merged.csv')\n",
    "CATALOGS_FOLDER = 'objeto_del_gasto.catalog'\n",
    "CATALOGS_FILE = 'objeto_del_gasto.catalog.xlsx'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if isdir(ITERATION_FOLDER):\n",
    "    raise ValueError('Please enter a unique iteration label')\n",
    "    \n",
    "mkdir(ITERATION_FOLDER)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding inspection\n",
    "\n",
    "Detect the file encodings of the input files using the `cChardet` utility library. __Warning:__ it's not always accurate. This is meant only as an indication only. In the end, encodings will be taken from `INPUT_FILES`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def detect_encodings():\n",
    "    \"\"\"Detect CSV file encoding with the cChardet library\"\"\"\n",
    "\n",
    "    try:\n",
    "        import cchardet as chardet\n",
    "    except ImportError:\n",
    "        cChardet = 'https://github.com/PyYoshi/cChardet'\n",
    "        print('Encoding inspection skipped: install %s', cChardet)\n",
    "        return\n",
    "\n",
    "    results = {}\n",
    "    results_file = join(OUTPUT_FOLDER, ITERATION_LABEL, 'encodings.detected.json')\n",
    "    \n",
    "    for year, file in sorted(INPUT_FILES.items()):\n",
    "        datafile = join(INPUT_FOLDER, file['name'])\n",
    "        \n",
    "        with open(datafile, 'rb') as f:\n",
    "            text = f.read()\n",
    "            \n",
    "        result = chardet.detect(text)\n",
    "        results.update({year: result})\n",
    "        print(year, 'Inspected', file['name'], result)\n",
    "    \n",
    "    with open(results_file, 'w+') as json:\n",
    "        json.write(dumps(results, indent=4))\n",
    "        print('\\nSaved encoding detection report to', results_file)\n",
    "        \n",
    "# detect_encodings()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def read_columns(file, encoding):\n",
    "    \"\"\"Return clean CSV file headers\"\"\"\n",
    "    \n",
    "    with open(file, encoding=encoding) as csv:\n",
    "        header = csv.readline()\n",
    "        return header.replace('\\n', '').split(',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def force_strings(columns):\n",
    "    \"\"\"Return string enforcement for each column of a CSV file\"\"\"\n",
    "    \n",
    "    for column in columns:\n",
    "        yield column, str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_csv_files():\n",
    "    \"\"\"Load raw data (CSV) files\"\"\"\n",
    "    \n",
    "    batch = {}\n",
    "    \n",
    "    for year, file in sorted(INPUT_FILES.items()):\n",
    "        filepath = join(INPUT_FOLDER, file['name'])\n",
    "        column_names = read_columns(filepath, file['encoding'])\n",
    "        column_types = dict(force_strings(column_names))\n",
    "        \n",
    "        batch[year] = read_csv(filepath, encoding=file['encoding'], dtype=column_types)\n",
    "        print('Loaded', file['name'], 'with encoding', file['encoding'])\n",
    "    \n",
    "    print()\n",
    "    stdout.flush()\n",
    "\n",
    "    for year in sorted(INPUT_FILES.keys()):\n",
    "        if year in REMOVE_INPUT_COLUMNS:\n",
    "            for column in REMOVE_INPUT_COLUMNS[year]:\n",
    "                try:\n",
    "                    del batch[year][column]\n",
    "                    print(year, 'deleted', column)\n",
    "                except KeyError:\n",
    "                    print(year, column, 'not found in', file['name'])\n",
    "\n",
    "        stdout.flush()\n",
    "\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def strip_cell_padding(batch):\n",
    "    for year in sorted(batch.keys()):\n",
    "        for column in batch[year].columns:\n",
    "            batch[year].rename(columns={column: column.strip()}, inplace=True)\n",
    "            batch[year][column] = batch[year][column].apply(lambda x: x.strip() if x is not nan else x)\n",
    "        print(year, 'stripped cell paddings')\n",
    "        stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def delete_empty_columns(batch):\n",
    "    for year in batch.keys():\n",
    "        for column in batch[year].columns:\n",
    "            if 'Unnamed:' in column:\n",
    "                try:\n",
    "                    del batch[year][column]\n",
    "                    print(year, column, 'deleted')\n",
    "                    stdout.flush()\n",
    "                except KeyError:\n",
    "                    pass  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def count_missing_values(batch):\n",
    "    collector = {}\n",
    "    table = []\n",
    "\n",
    "    for column in get_union_of_columns(batch):\n",
    "        row = {'Column': column}\n",
    "        collector.update({column: []})\n",
    "        \n",
    "        for year in batch.keys():\n",
    "            if column in batch[year].columns:\n",
    "                is_empty = batch[year][column].isnull()\n",
    "                empty_lines = batch[year].where(is_empty).dropna(how='all')\n",
    "                collector[column].extend(empty_lines.to_dict(orient='records'))\n",
    "                nb_empty_cells = len(empty_lines)\n",
    "            else:\n",
    "                nb_empty_cells = nan\n",
    "                \n",
    "            row.update({year: nb_empty_cells})\n",
    "            if nb_empty_cells not in (nan, 0):\n",
    "                print(year, 'found', nb_empty_cells, 'missing values in', column)\n",
    "\n",
    "        table.append(row)\n",
    "        \n",
    "    ordered_columns = ['Column']\n",
    "    ordered_columns.extend(sorted(batch.keys()))\n",
    "    empty_values_overview_table = DataFrame(table).reindex_axis(ordered_columns, axis=1)\n",
    "    \n",
    "    return empty_values_overview_table, collector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def count_duplicates(batch):\n",
    "    for year, df in sorted(batch.items()):\n",
    "        nb_duplicate_lines = df.duplicated().apply(lambda x: 1 if x is True else 0).sum()\n",
    "        print(year, 'found', nb_duplicate_lines, 'duplicate lines')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alias column names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_union_of_columns(batch):\n",
    "    union = set()\n",
    "    for year in batch.keys():\n",
    "        union = union | set(batch[year].columns)\n",
    "    return union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from yaml import load\n",
    "\n",
    "def load_aliases(file):\n",
    "    with open(file) as yaml:\n",
    "        aliases = load(yaml.read())\n",
    "        return aliases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def map_columns_to_aliases(batch, list_of_aliases):\n",
    "    for year in sorted(batch.keys()):\n",
    "        for column in sorted(batch[year].columns):\n",
    "            if not column in list_of_aliases:\n",
    "                for reference, aliases in list_of_aliases.items():\n",
    "                    if aliases:\n",
    "                        if column in aliases:\n",
    "                            batch[year].rename(columns={column: reference}, inplace=True)\n",
    "                            print(year, column, 'replaced with', reference)\n",
    "                            stdout.flush()\n",
    "                            break  \n",
    "                else:\n",
    "                    print(year, 'NO ALIAS REGISTERED FOR', column)\n",
    "                    stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_overview(batch):\n",
    "    table = []\n",
    "    \n",
    "    for column in get_union_of_columns(batch):\n",
    "        row = {'Column': column}\n",
    "        for year in batch.keys():\n",
    "            row.update({year: column in batch[year].columns})\n",
    "        table.append(row)\n",
    "        \n",
    "    ordered_columns = ['Column']\n",
    "    ordered_columns.extend(sorted(batch.keys()))\n",
    "    \n",
    "    overview = DataFrame(table).reindex_axis(ordered_columns, axis=1)\n",
    "    print('Column mapping overview: done')\n",
    "    return overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check expenditure sums\n",
    "\n",
    "There's a little cleaning to do on the amount columns (zeros represented by a dash). Assume thousands are seperated by a comma."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "EXPENDITURE_COLUMNS = [\n",
    "    'Ejercido', \n",
    "    'Devengado', \n",
    "    'Aprobado', \n",
    "    'Pagado', \n",
    "    'Modificado', \n",
    "    'Adefas', \n",
    "    'Ejercicio'\n",
    "]\n",
    "count = 0\n",
    "\n",
    "def clean_expenditure_columns(batch):\n",
    "    check_sums = []\n",
    "\n",
    "    for column in EXPENDITURE_COLUMNS:\n",
    "        row = {'Column': column}\n",
    "        \n",
    "        for year in sorted(batch.keys()):\n",
    "            try:\n",
    "                series = batch[year][column]\n",
    "                \n",
    "                # I'm assuming -' represents zero\n",
    "                series = series.apply(lambda x: '0' if x == '-' else x)\n",
    "                try:\n",
    "                    series = series.apply(lambda x: x.replace(',', '') if x is not nan else x)    \n",
    "                except AttributeError:\n",
    "                    if count < 10:\n",
    "                        print(year, column)\n",
    "                batch[year][column] = series.astype(float)\n",
    "                check_sum = batch[year][column].sum()\n",
    "                \n",
    "                print(year, 'cleaned and summed', column, '=', check_sum, 'pesos')\n",
    "                \n",
    "            except KeyError:\n",
    "                check_sum = nan\n",
    "                \n",
    "            row.update({year: check_sum})\n",
    "        \n",
    "        check_sums.append(row)\n",
    "\n",
    "    ordered_columns = ['Column']\n",
    "    ordered_columns.extend(sorted(batch.keys()))\n",
    "    return DataFrame(check_sums).reindex_axis(ordered_columns, axis=1)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objeto del Gasto Column split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from os.path import join\n",
    "\n",
    "def generate_catalog(file):\n",
    "    \n",
    "    catalog_ = {}\n",
    "    catalog_file = ExcelFile(file)\n",
    "    INDEX_COLUMN = 0\n",
    "    \n",
    "    for sheet in catalog_file.sheet_names:\n",
    "        if sheet != 'Concatenated':\n",
    "            name = slugify(sheet, separator='_')\n",
    "            output = join('objeto_del_gasto.catalog', name + '.csv')\n",
    "\n",
    "            df = catalog_file.parse(sheet).dropna()\n",
    "            index = df.columns[INDEX_COLUMN]\n",
    "\n",
    "            df[index] =  df[index].astype(str)\n",
    "            df.set_index(index, inplace=True)\n",
    "            df = df.groupby(df.index).first()\n",
    "            df.sort_index(inplace=True)\n",
    "            \n",
    "            message = 'Loaded catalog {sheet} into \"{name}\" ({nb} lines)'\n",
    "            parameters = dict(sheet=sheet, name=name, nb=len(df))\n",
    "\n",
    "            print(message.format(**parameters))\n",
    "            catalog_[name] = df['DESCRIPCION']\n",
    "    \n",
    "    print()\n",
    "    return catalog_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Note!__ Years are hard coded in the script below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def split_objeto_del_gasto(batch):\n",
    "    catalog = generate_catalog(CATALOGS_FILE)\n",
    "    missing_in_catalog = []\n",
    "    \n",
    "    def has_digits(n, N):\n",
    "        return not isinstance(n, float) and len(n) >= N \n",
    "            \n",
    "\n",
    "    def lookup(n, table, year):\n",
    "        try:\n",
    "            return catalog[table].loc[n]\n",
    "        except KeyError:\n",
    "            missing_in_catalog.append({'year': year, 'table': table, 'ID': n})\n",
    "            return nan\n",
    "        except TypeError:\n",
    "            # n is nan\n",
    "            return nan\n",
    "    \n",
    "    for year in sorted(batch.keys()):\n",
    "        if year == 2016:\n",
    "            print('Skipping', year, 'because the raw CSV already has the required columns')\n",
    "        \n",
    "        else:\n",
    "            objeto = batch[year]['Objeto del Gasto'].astype(str)\n",
    "\n",
    "            batch[year]['Capitulo'] = objeto.apply(lambda x: x[0] + '000' if x not in (nan, 'nan') else nan)\n",
    "            batch[year]['Concepto'] = objeto.apply(lambda x: x[:2] + '00' if x not in (nan, 'nan') else nan)\n",
    "            batch[year]['Descripción de Capitulo'] = batch[year]['Capitulo'].map(lambda x: lookup(x, 'capitulo', year))  \n",
    "            batch[year]['Descripción de Concepto'] = batch[year]['Concepto'].map(lambda x: lookup(x, 'concepto', year))  \n",
    "            \n",
    "            # Skip the LAST year of the dataset (currently 2016) it has split columns already\n",
    "            batch[year]['Partida Genérica'] = objeto.apply(lambda x: x[:3] if has_digits(x, 4) else nan)\n",
    "            batch[year]['Descripción de Partida Genérica'] = batch[year]['Partida Genérica'].map(lambda x: lookup(x, 'partida_generica', year))  \n",
    "            \n",
    "            if year not in (2008, 2009, 2010):\n",
    "                batch[year]['Partida Específica'] = objeto.apply(lambda x: x if has_digits(x, 5) else nan)\n",
    "                batch[year]['Descripción de Partida Específica'] = batch[year]['Partida Específica'].map(lambda x: lookup(x, 'partida_especifica', year) if has_digits(x, 5) else nan)  \n",
    "            else:\n",
    "                batch[year]['Partida Específica'] = nan\n",
    "                batch[year]['Descripción de Partida Específica'] = nan\n",
    "\n",
    "            print(year, 'broke down \"Objeto del Gasto\" column')\n",
    "        \n",
    "    return DataFrame(missing_in_catalog).drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prefix IDs \n",
    "Disambiguating sub-categories may require prefixing their IDs with their parents' IDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prefix_ids(batch):\n",
    "    for year in batch.keys():       \n",
    "        for hierarchy, levels in HIERARCHIES.items():\n",
    "            prefix = batch[year]['Ciclo'].apply(lambda x: '')\n",
    "            for n, level in enumerate(levels):\n",
    "                dash = '-' if n > 0 else ''\n",
    "                prefix = prefix + dash + batch[year][level]  \n",
    "                batch[year][level] = prefix\n",
    "                \n",
    "                print(year, 'prefixed', hierarchy, 'level', n, level)\n",
    "                stdout.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove unused columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def remove_unused_columns(batch):\n",
    "    for year, budget in batch.items():\n",
    "        for column in REMOVE_OUTPUT_COLUMNS:\n",
    "            try:\n",
    "                del budget[column]\n",
    "                print(year, 'deleted', column)\n",
    "            except KeyError:\n",
    "                pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def do_pipeline():\n",
    "\n",
    "    def echo_section(section):\n",
    "        print('\\n', section, '\\n')\n",
    "\n",
    "    echo_section('Loading files')\n",
    "    datasets = load_csv_files()\n",
    "    \n",
    "    echo_section('Delete empty columns')\n",
    "    delete_empty_columns(datasets)\n",
    "\n",
    "    echo_section('Stripping padding from cells')\n",
    "    strip_cell_padding(datasets)\n",
    "    \n",
    "    echo_section('Counting duplicate lines (NOT de-duplicating)')\n",
    "    count_duplicates(datasets)\n",
    "    \n",
    "    echo_section('Mapping column to aliases')\n",
    "    map_columns_to_aliases(datasets, COLUMN_ALIASES)\n",
    "\n",
    "    echo_section('Counting missing values')\n",
    "    missing_values_report, bad_records = count_missing_values(datasets)\n",
    "    \n",
    "    echo_section('Building column mapping overview')\n",
    "    column_mapping_report = build_overview(datasets)\n",
    "    \n",
    "    echo_section('Cleaning expenditure columns')\n",
    "    sums_report = clean_expenditure_columns(datasets)\n",
    "    \n",
    "    echo_section('Breaking down Objeto del Gasto column')\n",
    "    missing_catalog_ids = split_objeto_del_gasto(datasets)\n",
    "        \n",
    "    echo_section('Prefixing IDs of certain category hierarchies')\n",
    "    prefix_ids(datasets)\n",
    "\n",
    "    echo_section('Removing unused columns')\n",
    "    remove_unused_columns(datasets)\n",
    "\n",
    "    echo_section('Saving pipeline configuration')\n",
    "\n",
    "    reports_file = join(ITERATION_FOLDER, BASENAME + '.reports.xlsx')\n",
    "    writer = ExcelWriter(reports_file)    \n",
    "    missing_values_report.to_excel(writer, 'missing values', encoding='utf-8', index=False)\n",
    "    column_mapping_report.to_excel(writer, 'column mapping', encoding='utf-8', index=False)\n",
    "    sums_report.to_excel(writer, 'check sums', encoding='utf-8', index=False)\n",
    "    missing_catalog_ids.to_excel(writer, 'missing_catalog_IDs', encoding='utf-8', index=False)    \n",
    "    print('Saved 4 reports to', reports_file)    \n",
    "\n",
    "    aliases_file = join(ITERATION_FOLDER, BASENAME + '.aliases.json')\n",
    "    inputs_file = join(ITERATION_FOLDER, BASENAME + '.inputs.json')\n",
    "    levels_file = join(ITERATION_FOLDER, BASENAME + '.levels.json')\n",
    "    bad_records_file = join(ITERATION_FOLDER, BASENAME + '.missing.json')\n",
    "\n",
    "    with open(bad_records_file, 'w+') as json:\n",
    "        json.write(dumps(bad_records, indent=4))\n",
    "        \n",
    "    with open(aliases_file, 'w+') as json:\n",
    "        json.write(dumps(COLUMN_ALIASES, indent=4))\n",
    "        \n",
    "    with open(levels_file, 'w+') as json:\n",
    "        json.write(dumps(HIERARCHIES, indent=4))\n",
    "        \n",
    "    with open(inputs_file, 'w+') as json:\n",
    "        json.write(dumps(INPUT_FILES, indent=4))\n",
    "    \n",
    "    print('Saved input configuration to', inputs_file)    \n",
    "    print('Saved column aliases to', aliases_file) \n",
    "    print('Saved bad records (those with empty cells) to', bad_records_file)    \n",
    "    print('Saved hierarchy levels used for prefixing to', levels_file) \n",
    "    \n",
    "    echo_section('Pipeline run \"%s\" done' % ITERATION_LABEL)\n",
    "\n",
    "    return datasets, missing_catalog_ids, column_mapping_report, missing_values_report, sums_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "budgets, missing_ids, column_mapping, missing_values, sums = do_pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from gc import collect\n",
    "collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for year, budget in budgets.items():\n",
    "    filepath = MERGED_FILE.replace('merged', str(year))\n",
    "    budget.to_csv(filepath, encoding='utf-8', index=False)\n",
    "    print('Saved', filepath)\n",
    "    stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "merged = concat(list(budgets.values()))\n",
    "merged.to_csv(MERGED_FILE, encoding='utf-8', index=False)\n",
    "print('Saved merged dataset to', MERGED_FILE)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quality control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sorted(list(budget.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "budget.sample(n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "objeto_breakdown = [\n",
    "    'Ciclo', \n",
    "    'Capitulo', 'Concepto', \n",
    "    'Partida Específica', \n",
    "    'Partida Genérica'\n",
    "]\n",
    "budget[objeto_breakdown].sample(n=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('Total: missing', len(missing_ids), 'catalog IDs to breakdown the \"Objeto del Gasto\" column')\n",
    "print('Tables:', dict(missing_ids.groupby('table').count()['ID']))\n",
    "print('Years:', dict(missing_ids.groupby('year').count()['ID']))\n",
    "missing_ids.sample(n=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "column_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "missing_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "merged.sample(n=20) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(join(ITERATION_FOLDER, BASENAME + '.missing.json')) as file:\n",
    "    aliases = loads(file.read())\n",
    "aliases['Descripción de Fuente de Financiamiento']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "breakdown = [\n",
    "    'Ciclo', \n",
    "    'Capitulo', \n",
    "    'Concepto', \n",
    "    'Partida Genérica',        \n",
    "    'Partida Específica', \n",
    "    'Descripción de Capitulo',\n",
    "    'Descripción de Concepto', \n",
    "    'Descripción de Partida Genérica',\n",
    "    'Descripción de Partida Específica'\n",
    "]\n",
    "\n",
    "merged[breakdown].sample(n=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "merged.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "budget.where(budget['Descripción de Programa Presupuestario'] == 'PROSPERA Programa de Inclusión Social')['Programa Presupuestario'].dropna() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "budget.where(budget['Programa Presupuestario'] == '72')['Descripción de Programa Presupuestario'].dropna().unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "budget.groupby(['Programa Presupuestario'])['Programa Presupuestario'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
